<!DOCTYPE html>
<html lang="en">
<style>
    .newsletter-container {
        max-width: 900px;
        max-height: 300px;
        border-radius: 1em;
        display: flex;
        flex-direction: column;
        padding: 0.5em 0;
        background: #052B69;
        box-shadow: 0 4px 6px 0 rgba(0, 0, 0, 0.2);
        margin-bottom: 0.5em;
        margin-top: 0.5em;
    }
    .newsletter-container * {
        align-self: center;
        padding: 0;
    }
    .newsletter-form {
        text-align: center;
    }
    .newsletter-title {
        font-size: 0.5em;
        margin: 0;
        color: #fff;
        font-family: ;
    }
    .newsletter-text {
        font-size: 0.5em
        font-family: $lato;
        font-weight: 500;
        color: #fff;
        text-align: center;
    }
    .newsletter-email,
    .newsletter-submit {
        display: inline-block;
        border-radius: 4px;
        padding: 0;
        align-self: center;
    }
    .newsletter-email {
        font-family: $serif-primary;
        text-align: center;
        font-size: .9em;
    }
    .newsletter-submit {
        font-size: .6em;
        margin-left: 1em;
        padding: 0.5em;
        border: none;
        background: #F0DF29;
        text-transform: uppercase;
        font-weight: 600;
    }

    @media screen and (min-width: 400px) {
        .newsletter-title {
            font-size: 2.5em;
        }
        .newsletter-text {
            font-size: 1.5em;
        }
        .newsletter-submit {
            font-size: .7em;
        }
    }
</style>

  <!-- Support for Latex and inline Latex -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true},
    jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
    extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js"],
    TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
    // toggle numbering equations
    equationNumbers: {
    autoNumber: "AMS"
    }
    }
    });
  </script>
  <!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2020 -->
  ---

---
<!DOCTYPE html>
<html>
<head>
  <!-- Google: google-site-verification -->
  <meta name="google-site-verification" content="Igdg61QIdwtXyb4IaoCnImb9X5Sf2ZeJpCwK2cugP1E" />
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=yes">

  <title>Basics of Reinforcement Learning</title>

  
  <meta name="author" content="Vikas Nataraja">
  

  
  <meta name="description" content="Q-function, policies and rewards">
  

  <link rel="shortcut icon" type="image/png" href="/assets/img/icons/192x192.png">
  <link rel="shortcut icon" sizes="196x196" href="/assets/img/icons/192x192.png">
  <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png">
  <link rel="alternate" type="application/rss+xml" title="Machine Learning and Coffee Chronicles - Vikas Nataraja's blog about machine learning, coffee, books and so much more!" href="http://localhost:4000/feed.xml">

  

  

  
<!-- Google Analytics -->
<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-165638023-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->



  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/main.css">
    
  

  

  

  <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="Basics of Reinforcement Learning">
  

   
  <meta property="og:description" content="Q-function, policies and rewards">
  


  <meta property="og:type" content="website">

  
  <meta property="og:url" content="http://localhost:4000/2020-05-13-Basics-of-RL/">
  <link rel="canonical" href="http://localhost:4000/2020-05-13-Basics-of-RL/">
  

  
  <meta property="og:image" content="http://localhost:4000/assets/img/cn_tower.jpg">
  


  <!-- Twitter summary cards -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@vikasnataraja">
  <meta name="twitter:creator" content="@vikasnataraja">

  
  <meta name="twitter:title" content="Basics of Reinforcement Learning">
  

  
  <meta name="twitter:description" content="Q-function, policies and rewards">
  

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/cn_tower.jpg">
  

  

  
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Basics of Reinforcement Learning | Machine Learning and Coffee Chronicles</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Basics of Reinforcement Learning" />
<meta name="author" content="Vikas Nataraja" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to my very first blog on this revamped website! I decided to write this blog about Reinforcement Learning because it is an area of Artificial Intelligence that I am absolutely fascinated by! From teaching robots how to pick up cans from a table to playing Mario, the applications of RL are limitless. It is a highly active area of research (see Deepmind, Covariant AI) and there are still many things to be explored and discovered. And so in this post, I will attempt to explain the basics of RL and we will gradually move to more complicated concepts in other blog posts." />
<meta property="og:description" content="Welcome to my very first blog on this revamped website! I decided to write this blog about Reinforcement Learning because it is an area of Artificial Intelligence that I am absolutely fascinated by! From teaching robots how to pick up cans from a table to playing Mario, the applications of RL are limitless. It is a highly active area of research (see Deepmind, Covariant AI) and there are still many things to be explored and discovered. And so in this post, I will attempt to explain the basics of RL and we will gradually move to more complicated concepts in other blog posts." />
<link rel="canonical" href="http://localhost:4000/2020-05-13-Basics-of-RL/" />
<meta property="og:url" content="http://localhost:4000/2020-05-13-Basics-of-RL/" />
<meta property="og:site_name" content="Machine Learning and Coffee Chronicles" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-13T00:00:00-04:00" />
<script type="application/ld+json">
{"description":"Welcome to my very first blog on this revamped website! I decided to write this blog about Reinforcement Learning because it is an area of Artificial Intelligence that I am absolutely fascinated by! From teaching robots how to pick up cans from a table to playing Mario, the applications of RL are limitless. It is a highly active area of research (see Deepmind, Covariant AI) and there are still many things to be explored and discovered. And so in this post, I will attempt to explain the basics of RL and we will gradually move to more complicated concepts in other blog posts.","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020-05-13-Basics-of-RL/"},"headline":"Basics of Reinforcement Learning","dateModified":"2020-05-13T00:00:00-04:00","datePublished":"2020-05-13T00:00:00-04:00","author":{"@type":"Person","name":"Vikas Nataraja"},"url":"http://localhost:4000/2020-05-13-Basics-of-RL/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>
</html>


  <body>

    


    <nav class="navbar navbar-expand-md navbar-light fixed-top navbar-custom "><a class="navbar-brand navbar-brand-logo" href="http://localhost:4000/"><img alt="Machine Learning and Coffee Chronicles Logo" src="assets/img/icons/sidebar.png"/></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/blog">Blog</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/assets/docs/Vikas_Nataraja_CV.pdf">CV</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/about">About Me</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/contact">Contact</a>
          </li></ul>
  </div>

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4000/">
          <img alt="Navbar avatar" class="avatar-img" src="/assets/img/cn_tower.jpg" />
        </a>
      </div>
    </div>
  

</nav>


    ---

---
<!DOCTYPE html>
<html>
<!-- TODO this file has become a mess, refactor it -->






  <div id="header-big-imgs" data-num-img=1
    
    
    
      
      data-img-src-1="/assets/img/basics_rl/rl_cover.jpeg"
    
    
    
  ></div>


<header class="header-section has-img">

<div class="big-img intro-header">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Basics of Reinforcement Learning</h1>
      
        
      <h2 class="post-subheading">Q-function, policies and rewards</h2>
      
      

      
          <span class="post-meta">Posted on May 13, 2020</span>
          
            <!--- "ReadTime on GitHub Jekyll" (c) 2020 Ruby Griffith Ramirez, MIT License -->






  
  <span class="reader-time post-meta"><span class="d-none d-md-inline middot">&middot;</span> 8 minute read</span>


          
      
        </div>
      </div>
    </div>
  </div>
  <span class='img-desc'></span>
</div>

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Basics of Reinforcement Learning</h1>
      
        
      <h2 class="post-subheading">Q-function, policies and rewards</h2>
      
      

      
          <span class="post-meta">Posted on May 13, 2020</span>
          
            <!--- "ReadTime on GitHub Jekyll" (c) 2020 Ruby Griffith Ramirez, MIT License -->






  
  <span class="reader-time post-meta"><span class="d-none d-md-inline middot">&middot;</span> 8 minute read</span>


          
      
        </div>
      </div>
    </div>
  </div>
</div>
</header>



</html>


<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      <article role="main" class="blog-post" align="justify">
        <p>Welcome to my very first blog on this revamped website! I decided to write this blog about Reinforcement Learning because it is an area of Artificial Intelligence that I am absolutely fascinated by! From teaching robots how to pick up cans from a table to playing Mario, the applications of RL are limitless. It is a highly active area of research (see Deepmind, Covariant AI) and there are still many things to be explored and discovered. And so in this post, I will attempt to explain the basics of RL and we will gradually move to more complicated concepts in other blog posts.</p>

<h1 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h1>

<p>Simply put, Reinforcement Learning is where an agent in an environment is trying to maximize or optimize the way it behaves to achieve a certain <em>reward</em>. And as the name suggests, we use positive and negative reinforcements to teach the agent how to behave (i.e. what actions to take at states). The overall objective of the agent is to maximize the reward and needs to learn how to do that. That was a lot of fancy words and terms so let’s back up for a second and break it down.</p>

<p><strong>But wait, what is this "agent" and what is this "reward"?</strong></p>

<p>To answer that question, let’s consider an example:</p>

<p>You’re playing fetch with your dog and every time she brings the ball back to you, you give her a treat. Not that you should play this game for long because you would run out of treats pretty soon but in this scenario, the dog is the agent. She knows that every time she’s successful in bringing the ball back to you, she gets a treat. So you could say, the agent (the dog) is trying to get the maximum reward (treat) by taking some action (fetching the ball).</p>

<figure align="center">
  <img width="500" height="300" src="/assets/img/basics_rl/dog_fetch.jpg" alt="Dog Reinforcement Learning" />
    <figcaption> Dog playing fetch :) Source: KDNuggets</figcaption>
</figure>

<p>Mathematically, we can represent it as $r_t$ referring to reward at timestep $t$.</p>

<p>Now that we have some notion of a reward, let’s move on to a couple of extended topics: discount factor and discounted future cumulative rewards. Now, stay with me here, it may sound complicated but it’s really not. To explain these terms, let’s take another example - chess. In chess, the goal is to surround the opponent and make it impossible to escape. In other words, checkmate. To that end, you make moves (taking actions) that can perhaps result in you losing a pawn or another chess piece but may allow you to eventually win the game. So, in a sense, you are playing the long-game and thinking ahead even if it’s only a couple of moves ahead. In RL, that’s what the discount factor $\gamma$ is for - to decide how much importance should be given to future rewards compared to immediate rewards. Usually, the $\gamma$ value is set $0&lt;\gamma&lt;1$. Setting $\gamma=1$ means the agent will evaluate each of its actions based on the sum total of all its future rewards. <a href="https://stackoverflow.com/a/54346760/12623546">This StackOverflow answer</a> does very well to explain the difference $\gamma=0.9$ makes compared to $\gamma=0.99$. We will cover this in a later blog with some demonstrations in OpenAI Gym.</p>

<p>Using this discount factor $\gamma$ in our rewards, we can now describe our discounted future cumulative words as the reward that takes into account the discount factor. Mathematically, we use the discount factor exponentially as $\gamma^k$ to indicate that it affects the reward exponentially:</p>

<p>\begin{equation}
\label{eq:reward_future_cumulative}
  R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + … = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \nonumber
\end{equation}</p>

<p>So, all this equation is saying is the farther away a reward is, scale that reward appropriately. If we choose $\gamma=1$, then we weigh all future rewards equally. Choosing $\gamma=0.9$ weighs the rewards in the immediate future heavier than the rewards in the distant future. Depending on the environment we’re in, we tune our discount factor differently.</p>

<figure align="center">
  <img width="500" height="550" src="/assets/img/basics_rl/rewards_meme.jpg" alt="Future rewards meme" />
</figure>

<p><strong>Policies</strong></p>
<p>Policy is how the agent knows what action to take at different states. It describes how to act in a state by using probabilities. If at a state $s$, the probability of going left is 0.4 and the probability of going right is 0.6, then mathematically, we can write that as $\pi(s,left) = 0.4$ and $\pi(s,right) = 0.6$. This is describing the probability of taking some action $a$ at state $s$. It is providing a kind of mapping to go from state to action which is why it is also represented as $\pi(a|s,\theta)$ to indicate that we need to find an action given a state a meaning it is a stochastic policy. The $\theta$ is a parameter (or a vector of parameters) to fine-tune the learning because each state may have multiple actions and we need to find the best action for each state. Since this is a probability, the summation of all probabilities should be 1 i.e. $\sum_{a} \pi(s,a) = 1$. But that was an easy instance and in an environment where there may be tens or hundreds of states each with multiple actions, it becomes important to learn the policy and this can be achieved in many different ways and we will cover that in a later blog post.</p>

<p><strong>Value functions</strong></p>
<p>To learn about the correct actions to take, we need an expression that characterizes how good the quality of the possible actions are. Simply put, we need something to say <em>“this action is good”</em> or <em>“this action is not so good”</em>. If we do that, we can then simply say, take the action that results in the highest value out of those possibilities because remember, in RL, we don’t have a direct control over which states we end up in but rather we control our actions to make sure we end up in a certain state. But to actually know which action is good or bad, we use value functions that can quantify the quality of an action. Remember that $R_t$ is the future cumulative reward given by: $R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + … $. Using that, we can describe value functions as:</p>

<p>\begin{equation}
\label{eq:value_function}
  V_\pi(s) = \mathbb{E}_\pi [R_t|S_t = s] \nonumber
\end{equation}</p>

<p>\begin{equation}
\label{eq:q_function}
  Q_\pi(s,a) = \mathbb{E}_\pi [R_t|S_t = s, A_t = a] \nonumber
\end{equation}</p>

<p>Here, the $V(s)$ function is called the <strong>state-value</strong> because it is the expected return if we are in state $s$ at time $t$, $S_t=s$ and the $Q(s,a)$ function is called the <strong>action-value</strong> of a state-action pair which describes the value of taking an action $a$ at state $s$ at time $t$. But, we still need to go integrate the immediate rewards and the future discounted rewards and we do this by using the <strong>Bellman Equations</strong>. Let’s break down the value functions to understand the Bellman equations:</p>

<p>\begin{array}{lcl}
  V_\pi(s) &amp; = &amp; \mathbb{E}_\pi [R_t|S_t = s] \newline
  &amp; = &amp; \mathbb{E}_\pi [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + …|S_t = s] \newline
  &amp; = &amp; \mathbb{E}_\pi [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + …)|S_t = s] \newline
  &amp; = &amp; \mathbb{E}_\pi [R_{t+1} + \gamma R_{t+1}|S_t = s] \newline
  V_\pi(s) &amp; = &amp; \mathbb{E}_\pi [R_{t+1} + \gamma V(S_{t+1})|S_t = s]
\end{array}</p>

<p>Similarly, we can write the Bellman equations for the Q function as:</p>

<p>\begin{array}{lcl}
  Q_\pi(s,a) &amp; = &amp; \mathbb{E}_\pi [R_{t+1} + \gamma V(S_{t+1})|S_t = s, A_t = a] \newline
  Q_\pi(s,a) &amp; = &amp; \mathbb{E}_\pi [R_{t+1} + \gamma \mathbb{E_{a \sim \pi}} Q(S_{t+1},a)|S_t = s, A_t = a]
\end{array}</p>

<p>So now, we can use these value functions (Bellman equations) to get state-value and action-value at a timestep. Let’s say the agent is in state $s$ and decides to take an action $a$ and arrives in <em>next state</em> $s’$ and receives reward $r$. Then, it takes an action $a’$ to go to the next state $s’’$ and so on.</p>

<figure align="center">
  <img width="500" height="300" src="/assets/img/basics_rl/transition.png" alt="Q-learning update step" />
    <figcaption> Update step </figcaption>
</figure>

<p>But we need to do that recursively and update them on the go. To generalize this update step, we use $\pi(a \vert s)$ (policy gives us the probability of taking an action in a state) with our value function like this:</p>

<p>\begin{array}{lcl}
  V_\pi(s) = \sum_{a} \pi(a|s) Q_{\pi}(s,a) \newline
  Q_\pi(s,a) = R(s,a) + \gamma \sum_{s’} P_{ss’}^{a}, V_\pi(s’) \newline
  V_\pi(s) = \sum_{a} \pi(a|s) (R(s,a) + \gamma \sum_{s’} P_{ss’}^{a}, V_\pi(s’)) \newline
  Q_\pi(s,a) = R(s,a) + \gamma \sum_{s’} P_{ss’}^{a} \sum_{a’} \pi(a’|s’) Q_{\pi}(s’,a’)
\end{array}</p>

<p>The updated value functions give us the final pieces of the RL puzzle by actually updating the policy as the agent moves through the environment. But that was a lot of stuff and a lot of technical terms so let’s recap.</p>

<p><strong>Recap</strong></p>

<p>The agent is deployed in an environment that has some <strong>states $s$ and some actions $a$</strong>. While the agent doesn’t have direct control over which state it moves to, it does have control over the actions which when taken results in the transition to a new state $s’$.</p>

<figure align="center">
  <img width="650" height="300" src="/assets/img/basics_rl/rl_schematic.png" alt="Reinforcement Learning Schematic" />
    <figcaption>Schematic representation of an RL agent. Source: Sutton and Barto</figcaption>
</figure>

<p>Whenever the agent makes a move, it is rewarded with either positive or negative reinforcement called a <strong>reward $r$</strong>. The goal then becomes to maximize this reward by balancing immediate rewards that result in short-term positive rewards and future rewards that result in long-term positive rewards. The <strong>discount factor $\gamma$</strong> balances these two factors. To actually make a move and know how to make a move, the agent needs some instructions and guidance which is provided by the policy. It provides a probabilistic approach to maximizing rewards. Every time the agent enters a new state, the value function (state-value and action-value) is calculated to update the policy which then becomes the basis for the successive step. This is demonstrated by the figure above: the agent is in state $s_t$ and takes action $a_t$ at time $t$ which results in the transition to state $s_{t+1}$ and a reward $r_{t+1}$. Using this reward, the value function is updated in the policy and then repeats this process till the end goal (if any) is achieved.</p>

<p>The field of Reinforcement Learning is huge and this post has a lot of seemingly intimidating concepts especially for a beginner. But at the end of the day, it actually just boils down to some clever math and having these basics is super important and I can vouch for that. I’ve found that going over some other blog posts offering differently worded explanations always helps and I’ll attempt to explain more as we go. In the next blog post, we’ll explore <a href="https://gym.openai.com">OpenAI Gym</a>, a popular platform that allows for easy implementation of RL algorithms. See you there!</p>

<p>Cover Image credit: Big Hero 6 via Walt Disney Pictures</p>

      </article>

      
        <div class="blog-tags">
          Tags:
          
          
            <a href="/tags#rl">rl</a>
          
            <a href="/tags#robotics">robotics</a>
          
            <a href="/tags#ai-series">ai-series</a>
          
          
        </div>
      

      
        <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  
    <a href="https://twitter.com/intent/tweet?text=Basics+of+Reinforcement+Learning&url=http%3A%2F%2Flocalhost%3A4000%2F2020-05-13-Basics-of-RL%2F"
      class="btn btn-social-icon btn-twitter" title="Share on Twitter">
      <span class="fab fa-fw fa-twitter" aria-hidden="true"></span>
      <span class="sr-only">Twitter</span>
    </a>
  

  
    <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2F2020-05-13-Basics-of-RL%2F"
      class="btn btn-social-icon btn-facebook" title="Share on Facebook">
      <span class="fab fa-fw fa-facebook" aria-hidden="true"></span>
      <span class="sr-only">Facebook</span>
    </a>
  

  
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2020-05-13-Basics-of-RL%2F"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fab fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

</section>



      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2020-05-10-welcome/" data-toggle="tooltip" data-placement="top" title="Welcome to my blog!">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/2020-05-29-bookclub-educated-westover/" data-toggle="tooltip" data-placement="top" title="BookClub - Educated by Tara Westover">Next Post &rarr;</a>
        </li>
        
      </ul>
              
  <div class="disqus-comments">
  <div class="comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
	  var disqus_shortname = 'vikasnataraja';
	  /* ensure that pages with query string get the same discussion */
	  var url_parts = window.location.href.split("?");
	  var disqus_url = url_parts[0];
	  (function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	  })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</div>
  
  

  



    </div>
  </div>
</div>


    <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="/feed.xml" title="RSS">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">RSS</span>
    </a>
  </li><li class="list-inline-item">
    <a href="mailto:hnvikas14@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/vikasnataraja" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://twitter.com/vikasnataraja" title="Twitter">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Twitter</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/vikas-hanasoge-nataraja" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Vikas Nataraja
        &nbsp;&bull;&nbsp;
      
      2021

      
        &nbsp;&bull;&nbsp;
        <a href="http://localhost:4000/">Vikas Nataraja</a>
      

      
      </p>
      <!-- Please don't remove this, keep my open source work credited :) -->
      <p class="theme-by text-muted">
        Theme by
        <a href="https://beautifuljekyll.com">beautiful-jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


    
  
    
  <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/main.js"></script>
    
  







  </body>
</html>
