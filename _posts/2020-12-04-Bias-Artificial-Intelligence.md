---
layout: post
title: Bias in Artificial Intelligence
subtitle:
cover-img: /assets/img/bias_ai/msceleb_montage.jpg
tags: [machine-learning, bias, ai-series]
---

There is increasing scrutiny surrounding machine learning algorithms and how they can go wrong very badly. Facial recognition is a topic that has gathered momentum in recent years with companies like Clearview AI providing the technology to law enforcement to, according to their website, "track down hundreds of at-large criminals, including pedophiles, terrorists, and sex traffickers". Obviously, there are complications to this claim. Facial recognition might be the prominent issue that catches the eye but there are many more subtle issues that, if not addressed soon, could turn into a bigger problem.

# Facial Recognition

So, let's start with face recognition. This is not a new development by any means. Machine learning has long been used for the detection and recognition of faces. In 1996, [researchers at Carnegie Mellon developed a neural network](https://www.ri.cmu.edu/pub_files/pub1/rowley_henry_1996_3/rowley_henry_1996_3.pdf) that had up to 93% accuracy. Suffice to say, both machine learning and face detection technology have come a long way since then and is a highly active area of research. Clearview AI burst onto the scene with a simple message to law enforcement officials - "we can help you catch the bad guys". But, the way their model works is not entirely clear. They claim to only use public data but that is a huge umbrella these days. Social media sites including Facebook, Twitter, and Google account for more than 4 billion users worldwide. The data that comes out of those platforms is mammoth-sized and Clearview's model scrapes all the open internet for that data. Now, purely as a machine learning exercise, that seems great because surely more data is better? That is what I learned when I started working in the field. But, as with any dataset, there are caveats to that and the most obvious for face-based data is race or ethnicity. Clearview claims to have more than 3 billion images in its dataset but the underlying demographics are unclear. But the implications of anything other than absolutely perfect data are severe. People of color tend to be negatively affected in such circumstances.

<figure align="center">
  <img width="700" height="400" src="/assets/img/bias_ai/nature_megaface.jpg" alt="Facial Recognition montage">
    <figcaption itemprop="caption description"><a href="https://megapixels.cc/megaface/">MegaFace</a></figcaption>
</figure>


A U.S. government [study published by NIST](https://www.nist.gov/news-events/news/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software) in 2019 found alarming results. With a sample size of 189 algorithms ranging from Microsoft to Amazon, the study found that African-American and Asian faces were 10 to 100 times more likely to be misidentified than Caucasian faces. Native Americans came off even worse. Women were also likely to have higher false-positive rates than men. Think about that for a second. That means a woman or a person of color could be falsely identified as a criminal or a person of interest in an investigation by law enforcement. All this and I haven't even talked about privacy (or loss of privacy). The bias in data tends to skew machine learning algorithms in often unpredictable ways. Neural networks are touted as "black-boxes" to mean that the features that the model captures or types of relations it learns between its data points are hidden under the box and very difficult to interpret. As a result, the burden rests on the developer to identify and closely study the data before using it. What is the demographic, what is the age group, what is the gender, are there equal samples from each group, what is the model's effectiveness on each of them? There are so many questions that need to be addressed before even starting to work on the machine learning part.

# Twitter and the (in)famous auto-crop

Switching gears, let's talk about my favorite social medium - Twitter. I love Twitter because I get near real-time information about politics, soccer and so much more. It shows me what the trends are and more importantly to me, does not impose a responsibility to post selfies :) But the auto-crop feature which automatically crops any image you upload is probably my least favorite aspect because it always seems to miss the important parts of the image. The irony is that the [algorithm they use](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/Smart-Auto-Cropping-of-Images.html) is termed "Saliency Prediction", which is supposed to identify the most salient part of the picture such as a person's face instead of feet or the top of a building instead of the floor. There are some hilarious fails online but there are also serious implications to such an algorithm.

<blockquote align="center" class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Trying a horrible experiment...<br><br>Which will the Twitter algorithm pick: Mitch McConnell or Barack Obama? <a href="https://t.co/bR1GRyCkia">pic.twitter.com/bR1GRyCkia</a></p>&mdash; Tony ‚ÄúAbolish (Pol)ICE‚Äù Arcieri ü¶Ä (@bascule) <a href="https://twitter.com/bascule/status/1307440596668182528?ref_src=twsrc%5Etfw">September 19, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


For example, there can be racial undertones to the algorithm. It could learn to count a white person's face as "salient" rather than a person of color's. (In)Famously, that is exactly what happened with Twitter and there was [serious backlash](https://roguerocket.com/2020/09/21/twitter-auto-crop-bias/).

# Twitter's auto-cropping algorithm

Twitter [published a paper](https://arxiv.org/pdf/1801.05787.pdf) talking about pruning. Specifically, they introduce a method called Fisher pruning which aims to speed up computation. So, how does it work, exactly? Well, first, they build on a model called the [DeepGaze II](https://arxiv.org/pdf/1610.01563.pdf), which is an object recognition architecture based on VGG-19. Several convolutional layers are used for feature extraction whose feature maps are then upsampled and concatenated to a readout network that uses 1x1 convolutions as a projection to obtain point-wise non-linearity. Finally, a Gaussian filter is applied to blur the output with the addition of a center bias since viewers tend to focus on center pixels. The authors of the Twitter paper mention that *"This center bias is computed as the marginal log-probability of a fixation landing on a given pixel, $log Q(x, y)$, and is dataset dependent."* To seemingly overcome that, they instead bilinearly upsample the 1-D output of the readout network instead of upsampling individual feature maps, apply separable filters to obtain Gaussian blur, and then upsample and crop the output to match the size of the input image. They also employ different architectures and their proposed Fisher pruning to remove redundant parameters in feature maps and therefore speed up computation. In particular, the pruning algorithm takes in gradients as input and penalizes computational complexity. They ended up with pretty good results:

<figure align="center">
  <img width="700" height="800" src="/assets/img/bias_ai/twitter_results.png" alt="Twitter auto crop  results">
    <figcaption> Results from Twitter's model </figcaption>
</figure>

But, what's incredibly surprising is that the problem of racial bias is never addressed or discussed in the paper. When dealing with large amounts of data, especially social media pictures, it becomes important to pay attention to the underlying information. Now, because Twitter is perhaps not the first place people would post selfies or images of their family and friends, presumably the engineers at Twitter wanted something more generalizable, something that could accommodate to crop the right part of the image, be it a selfie on a mountain or a deep-dish pizza. [In a previous post, I discussed the issue of bias-variance tradeoff](https://vikasnataraja.github.io/2020-08-03-Bias-Variance-Machine-Learning) and that is something that I would assume the programmers looked at first. Trading accuracy for lower computation time is a common practice in ML and is exactly what the Twitter paper sets out to do with pruning. Running those models on a phone as an app requires incredibly fast computation and they cannot afford to lose even a tenth of a second because users would notice it. However, it cannot be an excuse to have miss vital information in the data itself.

[Twitter has already apologized](https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm) and vowed to work on it. I, for one, am a fan of Twitter and sincerely hope they do better. With the growing power of social media, ethical responsibilities will also increase. In the future, I will explore bias in more detail, delving deeper into the problems behind it and potential solutions. It is a very pertinent topic in these times and we need get better at it quickly.

Cover image credit: Microsoft Celebrity Dataset
